import json
import os

import optuna

from ML_classifier import SentimentClassifier, load_and_split_dataset
from OptunaOptimizer import objective, format_trial_output
from main import evaluate_accuracy, evaluate_model_performance


def extract_test_info(filename):
    """
    Extracts test information from the given filename.
    It determines whether the test is related to BERT or BART and extracts the test name.

    :param filename: The filename generated by the generate_filename function.
    :return: A tuple containing the test name and the type of test (BERT or BART).
    """
    test_type = ''
    test_name = ''

    # Check if the filename starts with 'Bert_' or 'Bart_'
    if filename.startswith('Bert_Case_Tagger_'):
        test_type = 'BERT CASE TAGGER'
        test_name = filename[len('Bert_Case_Tagger_'):]  # Extract the test name part
    elif filename.startswith('Bart_Case_Tagger_'):
        test_type = 'BART CASE TAGGER'
        test_name = filename[len('Bart_Case_Tagger_'):]  # Extract the test name part
    elif filename.startswith('Bert_'):
        test_type = 'BERT'
        test_name = filename[len('Bert_'):]  # Extract the test name part
    elif filename.startswith('Bart_'):
        test_type = 'BART'
        test_name = filename[len('Bart_'):]  # Extract the test name part
    else:
        test_name = filename  # No prefix, so the entire filename is the test name

    # Replace underscores with spaces (reversing the generate_filename logic)
    test_name = test_name.replace('_', ' ')
    test_name = test_name.replace('+', ' + ')
    test_name = test_name.replace('.pt', '')

    return test_type + " " + test_name, test_type


def convert_to_dict(scores):
    """
    Convert an array of scores to a dictionary with keys for negative, neutral, and positive.

    Args:
    scores (list or array): A list or array containing three numerical values.

    Returns:
    dict: A dictionary with the sentiment scores.
    """
    sentiment_labels = ['negative', 'neutral', 'positive']
    return {label: score for label, score in zip(sentiment_labels, scores)}


def process_tensors(directory):
    results = {}
    class_names = ['negative', 'neutral', 'positive']  # Class names corresponding to labels

    for filename in os.listdir(directory):
        if filename.endswith('.pt'):
            # Extract test information
            test_name, test_type = extract_test_info(filename)
            print(f"Optimizing: {test_name}, Type: {test_type}")

            # Set input size based on test type
            input_size = 768 if test_type == 'BERT' else 1024
            if test_type == 'BERT CASE TAGGER':
                input_size = 788
            elif test_type == 'BART CASE TAGGER':
                input_size = 1044

            # Load dataset
            dataset_path = os.path.join(directory, filename)

            # Optuna optimization
            study = optuna.create_study(direction="maximize")
            study.optimize(lambda trial: objective(trial, dataset_path, input_size), n_trials=25)
            best_params = study.best_params
            # Get Starter params for averaging
            avg_train_accuracy = 0
            avg_test_accuracy = 0
            avg_train_precision = [0, 0, 0]
            avg_train_recall = [0, 0, 0]
            avg_train_f1_score = [0, 0, 0]
            avg_test_precision = [0, 0, 0]
            avg_test_recall = [0, 0, 0]
            avg_test_f1_score = [0, 0, 0]
            num_tests= 10
            for i in range(num_tests):
                # Initialize model with best hyperparameters
                model = SentimentClassifier(input_size=input_size, num_layers=best_params['num_layers'],
                                            layer_reduction=best_params['layer_reduction'],
                                            learning_rate=best_params['lr'])

                # Load data
                train_dataset, test_dataset = load_and_split_dataset(dataset_path, 0.8)

                # Train model with best hyperparameters
                model.fit(train_dataset, epochs=30, batch_size=best_params['batch_size'])

                # Evaluate accuracy, precision, recall, and F1-score
                avg_train_accuracy += evaluate_accuracy(model, train_dataset)
                avg_test_accuracy += evaluate_accuracy(model, test_dataset)
                train_eval = evaluate_model_performance(model, train_dataset)
                test_eval = evaluate_model_performance(model, test_dataset)
                for j in range(3):  # Assuming you have 3 classes
                    avg_train_precision[j] += train_eval[0][j]
                    avg_train_recall[j] += train_eval[1][j]
                    avg_train_f1_score[j] += train_eval[2][j]
                    avg_test_precision[j] += test_eval[0][j]
                    avg_test_recall[j] += test_eval[1][j]
                    avg_test_f1_score[j] += test_eval[2][j]
            # Average out the results
            train_accuracy = round(avg_train_accuracy / num_tests, 2)
            test_accuracy = round(avg_test_accuracy / num_tests, 2)
            train_precision = [round(x / num_tests, 2) for x in avg_train_precision]
            train_recall = [round(x / num_tests, 2) for x in avg_train_recall]
            train_f1_score = [round(x / num_tests, 2) for x in avg_train_f1_score]
            test_precision = [round(x / num_tests, 2) for x in avg_test_precision]
            test_recall = [round(x / num_tests, 2) for x in avg_test_recall]
            test_f1_score = [round(x / num_tests, 2) for x in avg_test_f1_score]
            # Formated train results
            formatted_train_precision = convert_to_dict(train_precision)
            formatted_train_recall = convert_to_dict(train_recall)
            formatted_train_f1_score = convert_to_dict(train_f1_score)

            # Formated test results
            formatted_precision = convert_to_dict(test_precision)
            formatted_recall = convert_to_dict(test_recall)
            formatted_f1_score = convert_to_dict(test_f1_score)
            best_params["epochs"] = 30
            # Store results
            results[test_name] = {
                "Best Hyperparameters": best_params,
                "Train Accuracy": train_accuracy,
                "Test Accuracy": test_accuracy,
                "Train Precision": formatted_train_precision,
                "Train Recall": formatted_train_recall,
                "Train F1 Score": formatted_train_f1_score,
                "Test Precision": formatted_precision,
                "Test Recall": formatted_recall,
                "Test F1 Score": formatted_f1_score
            }

        # Write results to JSON file
    with open('Eval_results.json', 'w') as json_file:
        json.dump(results, json_file, indent=4)


if __name__ == "__main__":
    process_tensors("./Tensor_Datasets")
